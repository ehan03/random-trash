{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark\n",
    "\n",
    "Notebook of messing around with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below taken from Spark By Examples website to validate installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "# Columns\n",
    "columns = [\"language\",\"users_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mess around on Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"house price practice\")\n",
    "    .setMaster(\"local\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.csv(\"./data/train.csv\", inferSchema=True, header=True)\n",
    "train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count(), len(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# null counts\n",
    "train_df.agg(\n",
    "    *[F.count_if(F.isnull(c)).alias(c) for c in train_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+---------+----------+------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|CabinZone|FamilySize| Title|\n",
      "+--------+------+------+----+-------+--------+---------+----------+------+\n",
      "|       0| Lower|  male|22.0|   7.25|       S|     NULL|         2|    Mr|\n",
      "|       1| Upper|female|38.0|71.2833|       C|        C|         2|   Mrs|\n",
      "|       1| Lower|female|26.0|  7.925|       S|     NULL|         1|  Miss|\n",
      "|       1| Upper|female|35.0|   53.1|       S|        C|         2|   Mrs|\n",
      "|       0| Lower|  male|35.0|   8.05|       S|     NULL|         1|    Mr|\n",
      "|       0| Lower|  male|NULL| 8.4583|       Q|     NULL|         1|    Mr|\n",
      "|       0| Upper|  male|54.0|51.8625|       S|        E|         1|    Mr|\n",
      "|       0| Lower|  male| 2.0| 21.075|       S|     NULL|         5|Master|\n",
      "|       1| Lower|female|27.0|11.1333|       S|     NULL|         3|   Mrs|\n",
      "|       1|Middle|female|14.0|30.0708|       C|     NULL|         2|   Mrs|\n",
      "+--------+------+------+----+-------+--------+---------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df2 = (\n",
    "    train_df\n",
    "    .withColumn(\"CabinZone\", F.substring(\"Cabin\", 1, 1))\n",
    "    .withColumn(\"FamilySize\", F.col(\"SibSp\") + F.col(\"Parch\") + 1)\n",
    "    .withColumn(\"Title\", F.regexp_extract(F.col(\"Name\"), r\", (.*?)\\. \", 1))\n",
    "    .withColumn(\"Title\", F.when(F.col(\"Title\").isin([\"Mr\", \"Mrs\", \"Miss\", \"Master\"]), F.col(\"Title\")).otherwise(\"Other\"))\n",
    "    .withColumn(\"Pclass\", \n",
    "                F.when(F.col(\"Pclass\") == \"1\", \"Upper\")\n",
    "                .when(F.col(\"Pclass\") == \"2\", \"Middle\")\n",
    "                .when(F.col(\"Pclass\") == \"3\", \"Lower\"))\n",
    "    .drop(\"Cabin\", \"SibSp\", \"Parch\", \"PassengerId\", \"Ticket\", \"Name\")\n",
    ")\n",
    "train_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+----+--------+---------+----------+-----+\n",
      "|Survived|Pclass|Sex|Age|Fare|Embarked|CabinZone|FamilySize|Title|\n",
      "+--------+------+---+---+----+--------+---------+----------+-----+\n",
      "|       0|     0|  0|177|   0|       2|      687|         0|    0|\n",
      "+--------+------+---+---+----+--------+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df2.agg(\n",
    "    *[F.count_if(F.isnull(c)).alias(c) for c in train_df2.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- CabinZone: string (nullable = true)\n",
      " |-- FamilySize: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [field for (field, dtype) in train_df2.dtypes if dtype == \"string\"]\n",
    "cat_idx_cols = [x + \"Index\" for x in cat_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=cat_cols, outputCols=cat_idx_cols, handleInvalid=\"keep\")\n",
    "imputer = Imputer(strategy=\"median\", inputCols=[\"Age\"], outputCols=[\"AgeImputed\"])\n",
    "\n",
    "num_cols = [field for (field, dtype) in train_df2.dtypes if (dtype != \"string\" and field not in [\"Survived\", \"Age\"])]\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=cat_idx_cols + num_cols, \n",
    "                                outputCol=\"features\", handleInvalid=\"keep\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Survived\")\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, imputer, vec_assembler, rf])\n",
    "pipelineModel = pipeline.fit(train_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|            features|Survived|Prediction|\n",
      "+--------------------+--------+----------+\n",
      "|(7,[3,5,6],[8.0,7...|       0|       0.0|\n",
      "|[1.0,1.0,1.0,0.0,...|       1|       1.0|\n",
      "|[0.0,1.0,0.0,8.0,...|       1|       1.0|\n",
      "|[1.0,1.0,0.0,0.0,...|       1|       1.0|\n",
      "|(7,[3,5,6],[8.0,8...|       0|       0.0|\n",
      "|[0.0,0.0,2.0,8.0,...|       0|       0.0|\n",
      "|[1.0,0.0,0.0,3.0,...|       0|       0.0|\n",
      "|[0.0,0.0,0.0,8.0,...|       0|       0.0|\n",
      "|[0.0,1.0,0.0,8.0,...|       1|       1.0|\n",
      "|[2.0,1.0,1.0,8.0,...|       1|       1.0|\n",
      "|[0.0,1.0,0.0,6.0,...|       1|       1.0|\n",
      "|[1.0,1.0,0.0,0.0,...|       1|       1.0|\n",
      "|(7,[3,5,6],[8.0,8...|       0|       0.0|\n",
      "|(7,[3,5,6],[8.0,3...|       0|       0.0|\n",
      "|[0.0,1.0,0.0,8.0,...|       0|       1.0|\n",
      "|[2.0,1.0,0.0,8.0,...|       1|       1.0|\n",
      "|[0.0,0.0,2.0,8.0,...|       0|       0.0|\n",
      "|[2.0,0.0,0.0,8.0,...|       1|       0.0|\n",
      "|[0.0,1.0,0.0,8.0,...|       0|       1.0|\n",
      "|[0.0,1.0,1.0,8.0,...|       1|       1.0|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train = pipelineModel.transform(train_df2)\n",
    "pred_train.select(\"features\", \"Survived\", \"Prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationEvaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"prediction\",\n",
    "    labelCol=\"Survived\",\n",
    "    metricName=\"areaU\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random-trash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
